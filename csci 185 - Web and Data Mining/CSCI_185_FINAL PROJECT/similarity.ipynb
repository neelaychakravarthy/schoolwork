{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/neelaychakravarthy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/neelaychakravarthy/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/neelaychakravarthy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/neelaychakravarthy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to find synonyms and remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_synonyms(words):\n",
    "    synonyms = set()\n",
    "    \n",
    "    for word in words:\n",
    "        for syn in wordnet.synsets(word):\n",
    "            for lemma in syn.lemmas():\n",
    "                synonyms.add(lemma.name())  # Add the synonyms to the set\n",
    "    \n",
    "    return list(synonyms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(query):\n",
    "    indiv_tokens = word_tokenize(query)\n",
    "    stop = set(stopwords.words('english'))\n",
    "    processed_tokens = []\n",
    "\n",
    "    for word in indiv_tokens:\n",
    "            if word.lower() not in stop:\n",
    "                processed_tokens.append(word)\n",
    "    return processed_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating document set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Song Name', 'URL', 'Artist', 'Lyrics'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "df = pd.read_csv('Lyrics_complete.csv', encoding='latin1')\n",
    "print(df.columns)\n",
    "\n",
    "songs = 'song_lyrics'\n",
    "\n",
    "documents = []\n",
    "\n",
    "\n",
    "if not os.path.exists(songs):\n",
    "    os.makedirs(songs)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    song_name = row['Song Name']  \n",
    "    lyrics = row['Lyrics']  \n",
    "\n",
    "    documents.append(remove_stop_words(lyrics))\n",
    "   \n",
    "    filename = \"\".join([c for c in song_name if c.isalpha() or c.isdigit() or c==' ']).rstrip()\n",
    "    \n",
    "\n",
    "    filepath = os.path.join(songs, f'{filename}.txt')\n",
    "    \n",
    "  \n",
    "    with open(filepath, 'w', encoding='utf-8') as file:\n",
    "        file.write(lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = []\n",
    "for doc in documents:\n",
    "    c = \"\"\n",
    "    for word in doc:\n",
    "        c += \" \" + word\n",
    "    songs.append(c)\n",
    "# print(songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"i want romance and petals and sex \"\n",
    "q = remove_stop_words(query)\n",
    "l = find_synonyms(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['want', 'romance', 'petals', 'sex']\n",
      "['court', 'dally', 'Latin', 'wish', 'deficiency', 'arouse', 'gender', 'romance', 'romanticism', 'philander', 'excite', 'petal', 'privation', 'butterfly', 'sex', 'mash', 'woo', 'sexuality', 'deprivation', 'neediness', 'love_affair', 'sexual_urge', 'solicit', 'flower_petal', 'turn_on', 'coquet', 'flirt', 'Latinian_language', 'need', 'Romance', 'require', 'wind_up', 'sexual_practice', 'sexual_activity', 'want', 'sex_activity', 'wishing', 'coquette', 'desire', 'Romance_language', 'chat_up', 'lack', 'love_story']\n"
     ]
    }
   ],
   "source": [
    "print(q)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1243)\t0.030563651433216946\n",
      "  (0, 994)\t0.028055852369096322\n",
      "  (0, 3072)\t0.013814378501687357\n",
      "  (0, 1319)\t0.028055852369096322\n",
      "  (0, 4085)\t0.02627654061666212\n",
      "  (0, 1274)\t0.028055852369096322\n",
      "  (0, 4135)\t0.0219894298001073\n",
      "  (0, 1548)\t0.017702318983552477\n",
      "  (0, 716)\t0.030563651433216946\n",
      "  (0, 4554)\t0.030563651433216946\n",
      "  (0, 2089)\t0.0219894298001073\n",
      "  (0, 1705)\t0.01697383167186605\n",
      "  (0, 375)\t0.030563651433216946\n",
      "  (0, 1744)\t0.023768741552541498\n",
      "  (0, 5032)\t0.01697383167186605\n",
      "  (0, 341)\t0.022815320597510194\n",
      "  (0, 4371)\t0.030563651433216946\n",
      "  (0, 2851)\t0.030563651433216946\n",
      "  (0, 1567)\t0.028055852369096322\n",
      "  (0, 2375)\t0.030563651433216946\n",
      "  (0, 2545)\t0.01852820978095537\n",
      "  (0, 3149)\t0.030563651433216946\n",
      "  (0, 2537)\t0.028055852369096322\n",
      "  (0, 515)\t0.061827865147088414\n",
      "  (0, 2331)\t0.020609288382362805\n",
      "  :\t:\n",
      "  (101, 2657)\t0.1545061375283335\n",
      "  (101, 3052)\t0.1545061375283335\n",
      "  (101, 1180)\t0.1545061375283335\n",
      "  (101, 4005)\t0.1545061375283335\n",
      "  (101, 2761)\t0.1545061375283335\n",
      "  (101, 650)\t0.1545061375283335\n",
      "  (101, 3538)\t0.1545061375283335\n",
      "  (101, 3370)\t0.1545061375283335\n",
      "  (101, 1470)\t0.1545061375283335\n",
      "  (101, 3379)\t0.1545061375283335\n",
      "  (101, 3805)\t0.1545061375283335\n",
      "  (101, 3802)\t0.309012275056667\n",
      "  (101, 1767)\t0.1545061375283335\n",
      "  (101, 255)\t0.1545061375283335\n",
      "  (101, 1150)\t0.1545061375283335\n",
      "  (101, 2518)\t0.1545061375283335\n",
      "  (101, 1078)\t0.1545061375283335\n",
      "  (101, 1617)\t0.14470730544472166\n",
      "  (101, 991)\t0.14470730544472166\n",
      "  (101, 3999)\t0.1308966273018167\n",
      "  (101, 1185)\t0.14470730544472166\n",
      "  (101, 5083)\t0.12564605115597396\n",
      "  (101, 5063)\t0.1045607565296942\n",
      "  (101, 3050)\t0.07283065135018062\n",
      "  (101, 4963)\t0.06893368338737618\n"
     ]
    }
   ],
   "source": [
    "c = \"\"\n",
    "for word in l:\n",
    "    c += \" \" + word\n",
    "songs.append(c)\n",
    "    \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "sample = [\"sample sample sample sample\", \"hello it's me\"]\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(songs)\n",
    "print(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between documents is: \n",
      " [[1.         0.04906253 0.04175473 ... 0.09291161 0.06177973 0.00334949]\n",
      " [0.04906253 1.         0.05154628 ... 0.03256964 0.03240265 0.0017647 ]\n",
      " [0.04175473 0.05154628 1.         ... 0.01681601 0.02000859 0.        ]\n",
      " ...\n",
      " [0.09291161 0.03256964 0.01681601 ... 1.         0.0274431  0.        ]\n",
      " [0.06177973 0.03240265 0.02000859 ... 0.0274431  1.         0.        ]\n",
      " [0.00334949 0.0017647  0.         ... 0.         0.         1.        ]]\n",
      "\n",
      "\n",
      "Ranked order of documents: [ 8 26 18  9 83 46 47 89 31 73 11 69 42 13 16 77 55 37 84 49 86 71 81 30\n",
      " 23 66 34 29 60 90 43 17 51 85 36 39 25 67 19  6 93 45 57  0 21 22 96 59\n",
      " 78  1 38 35  5 12 92 80 41 62 91 10 82 14 79 44 76  4 87 88  3  2 94 95\n",
      " 97  7 75 20 15 40 48 98 50 33 52 53 54 32 56 28 58 27 61 63 64 65 24 68\n",
      " 70 72 74 99]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarities = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "print('Cosine similarity between documents is: \\n', cosine_similarities)\n",
    "print('\\n')\n",
    "\n",
    "num_rows, num_columns = cosine_similarities.shape\n",
    "ranked_order = np.argsort(-cosine_similarities[:num_rows-1, num_columns-1], axis = 0)\n",
    "\n",
    "print('Ranked order of documents:', ranked_order)\n",
    "ranked_songs = []\n",
    "artists = []\n",
    "for item in ranked_order:\n",
    "    ranked_songs.append(df.loc[item]['Song Name'])\n",
    "    artists.append(df.loc[item]['Artist'])\n",
    "# print(ranked_songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i wish i hated you by Ariana Grande\n",
      "I Want It That Way by Backstreet Boys\n",
      "TEXAS HOLD 'EM by Beyonce\n",
      "supernatural by Ariana Grande\n",
      "Tejano Blue by Cigarettes After Sex\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"{ranked_songs[i]} by {artists[i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
